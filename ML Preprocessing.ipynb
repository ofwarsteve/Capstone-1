{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Preprocessing\n",
    "Steve Donahue, www.github.com/sdonahue0132\n",
    "\n",
    "This notebook uses functions similar to those found in the EDA + Simulation Data Preprocessing notebook.  When run, it produces additional data that can be used for additional performance evaluation for the ML classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries #\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import math\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from functools import reduce\n",
    "import warnings\n",
    "from random import randint\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VIGRX, define functions to handle weekly moving averages and multiweek moving averages\n",
    "\n",
    "def CSV_Formatter (folder_name, filename):\n",
    "    df_init = pd.read_csv('raw_data/' + str(folder_name) +'/' + str(filename) +'.csv')\n",
    "    df = df_init.set_index('Date')[['Close']]\n",
    "    df.columns = [str(filename)]\n",
    "    df['Next Day Values'] = df[filename].shift(-1)\n",
    "    df['Percentages'] = df['Next Day Values']/df[str(filename)]\n",
    "    df_final = df[['Percentages']]\n",
    "    df_final.columns = [str(filename)]\n",
    "    return(df[[filename]], df_final)\n",
    "    \n",
    "prices, percentages = CSV_Formatter('Stock_Indices', 'VIGRX')\n",
    "\n",
    "def weekly_master(df, cutoff_date):\n",
    "    \n",
    "# A little setup is necessary to ensure fidelity across weekly data.  In the original yahoo finance data downloads,\n",
    "# Weekends and holidays are not counted in the Datetime Index.  The following code creates a working df that\n",
    "# Can be broken into calendar weeks at regular 7 day intervals, to better reflect paycheck contributions and \n",
    "# subsequent analyses can be done on a week to week basis.\n",
    "\n",
    "# Note that for days where data is unavailable, I've filled in the value 1, since the method of assessing portfolios\n",
    "# Is multiplication across daily percentage changes.  In this way, days when no percent changes are documented do not\n",
    "# affect the value of the investment.\n",
    "\n",
    "    # NOTE THAT THIS FUNCTION WILL ONLY ACCOMODATE DATAFRAMES WITH UP TO 8 COLUMNS AS WRITTEN! #\n",
    "\n",
    "    cutoff_date = pd.to_datetime(cutoff_date)\n",
    "    cutoff = cutoff_date - datetime.timedelta(days=cutoff_date.weekday())\n",
    "    \n",
    "    if cutoff_date.weekday() == 6:\n",
    "        td = timedelta(1)\n",
    "        cutoff_date = cutoff_date - td\n",
    "    \n",
    "    datelist = pd.to_datetime(df.index.values)\n",
    "    df['Datetime'] = datelist\n",
    "    df_timed = df.set_index('Datetime')\n",
    "    labels = df_timed.columns\n",
    "\n",
    "    df_segmented = pd.DataFrame(columns = labels)\n",
    "\n",
    "    daterange = int(str(df_timed.index.max() - df_timed.index.min()).replace(\" days 00:00:00\", ''))\n",
    "    all_dates = pd.date_range(df_timed.index.min(), periods=daterange).tolist()\n",
    "    \n",
    "    index_df = pd.DataFrame(all_dates)\n",
    "    index_df.columns = ['Datetime']\n",
    "    \n",
    "    working_df = index_df.merge(df_timed, how = 'outer', left_on = 'Datetime', right_on ='Datetime')\n",
    "    working_df = working_df.fillna(1).set_index('Datetime').sort_values('Datetime', ascending = False)\n",
    "    working_df = working_df.loc[working_df.index <= cutoff]\n",
    "\n",
    "    days = len(working_df.index)\n",
    "    number_of_weeks = int(np.floor(days/7))\n",
    "\n",
    "    df_progress = pd.DataFrame(index=[0,1,2,3,4])\n",
    "    weekly_eval = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, number_of_weeks):\n",
    "        portfolio_segment = working_df.iloc[i*7+1:(i+1)*7]\n",
    "        \n",
    "        products =[]\n",
    "        prod_1 = portfolio_segment.iloc[:, 0].product()\n",
    "        products.append(prod_1)\n",
    "\n",
    "        if len(labels) > 1:\n",
    "            prod_2 = portfolio_segment.iloc[:, 1].product()\n",
    "            products.append(prod_2)\n",
    "        if len(labels) > 2:        \n",
    "            prod_3 = portfolio_segment.iloc[:, 2].product()\n",
    "            products.append(prod_3)\n",
    "        if len(labels) > 3:\n",
    "            prod_4 = portfolio_segment.iloc[:, 3].product()\n",
    "            products.append(prod_4)\n",
    "        if len(labels) > 4:\n",
    "            prod_5 = portfolio_segment.iloc[:, 4].product()\n",
    "            products.append(prod_5)\n",
    "        if len(labels) > 5:\n",
    "            prod_6 = portfolio_segment.iloc[:, 5].product()\n",
    "            products.append(prod_6)\n",
    "        if len(labels) > 6:\n",
    "            prod_7 = portfolio_segment.iloc[:, 6].product()\n",
    "            products.append(prod_7)\n",
    "        if len(labels) > 7:\n",
    "            prod_8 = portfolio_segment.iloc[:, 7].product()\n",
    "            products.append(prod_8)\n",
    "   \n",
    "    # Can we just make this into a loop?\n",
    "    # For j in range(0, len(labels)):\n",
    "    #    temp_prod = portfolio_segment.iloc[:, j].product()\n",
    "    #    products.append(temp_prod)\n",
    "\n",
    "        td = timedelta(1)\n",
    "        weekly_eval[str(portfolio_segment.index[(cutoff + td).weekday()]).replace(\"00:00:00\", '')] = products\n",
    "    \n",
    "    weekly_eval = weekly_eval.T\n",
    "    weekly_eval.columns = labels\n",
    "    weekly_eval.index = pd.to_datetime(weekly_eval.index)\n",
    "    weekly_eval.index.name = 'Date'\n",
    "    weekly_eval.sort_index()\n",
    "\n",
    "    return(weekly_eval)\n",
    "\n",
    "def timed_eval(keyword, df, num):\n",
    "\n",
    "    title = keyword + ' ' + str(num) + '_Week_Avg' \n",
    "    moving_avg = []\n",
    "\n",
    "    for i in range(int(round(df.shape[0]))):\n",
    "        selection = df.iloc[i+1: i+ 1 + num, :]\n",
    "        prod = selection.iloc[:, 0].product()\n",
    "        moving_avg = np.append(moving_avg, prod)\n",
    "    \n",
    "    df[title]= moving_avg\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fidelity US 500, 'FUSEX', was rolled into another index, 'FXAIX' in November 2018.  \n",
    "# This cell joins the two records at the transition point for.... fidelity.\n",
    "\n",
    "(FXAIX, FXAIX_final) = CSV_Formatter('Stock_Indices', 'FXAIX')\n",
    "(FUSEX, FUSEX_final) = CSV_Formatter('Stock_Indices', 'FUSEX')\n",
    "\n",
    "FUSEX = pd.merge( FUSEX_final, FXAIX_final, how = 'outer', on = 'Date')\n",
    "\n",
    "FUSEX = pd.DataFrame(FUSEX['FUSEX'].fillna(FUSEX['FXAIX']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files for STOCK INDICES #\n",
    "\n",
    "(PREIX, PREIX_final) = CSV_Formatter('Stock_Indices', 'PREIX')\n",
    "(SWPPX, SWPPX_final) = CSV_Formatter('Stock_Indices', 'SWPPX')\n",
    "#(FUSEX, FUSEX_final) = CSV_Formatter('Stock_Indices', 'FUSEX_plus')\n",
    "(VFINX, VFINX_final) = CSV_Formatter('Stock_Indices', 'VFINX')\n",
    "(VIGRX, VIGRX_final) = CSV_Formatter('Stock_Indices', 'VIGRX')\n",
    "\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "dfstock_values = [PREIX[['PREIX']], SWPPX[['SWPPX']], FUSEX[['FUSEX']], VFINX[['VFINX']], VIGRX[['VIGRX']]]\n",
    "dfstock_finals = [PREIX_final, SWPPX_final, FUSEX, VFINX_final, VIGRX_final]\n",
    "\n",
    "\n",
    "#stock_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), dfstock_values)\n",
    "stock_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), dfstock_finals)\n",
    "\n",
    "pd.merge(stock_increments_df, FUSEX_final, how = 'outer')\n",
    "stock_weekly = weekly_master(stock_increments_df, '2019-02-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stock_weekly\n",
    "\n",
    "df = df[['PREIX', 'SWPPX', 'FUSEX', 'VFINX']]\n",
    "\n",
    "df['Average'] = (df['PREIX'] + df['SWPPX'] + df['FUSEX'] + df['VFINX'])/4\n",
    "\n",
    "stocks = pd.DataFrame()\n",
    "stocks.index_name = df.index.name\n",
    "stocks['Stock_increments'] = df.Average\n",
    "\n",
    "stocks.index = df.index\n",
    "\n",
    "stocks_2 = timed_eval('Stocks', stocks, 2)\n",
    "stocks_3 = timed_eval('Stocks', stocks_2, 3)\n",
    "stocks_6 = timed_eval('Stocks', stocks_3, 6)\n",
    "stocks_9 = timed_eval('Stocks', stocks_6, 9)\n",
    "stocks_12 = timed_eval('Stocks', stocks_9, 12)\n",
    "stocks_df = timed_eval('Stocks', stocks_12, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files for Intermediate Term Bonds #\n",
    "\n",
    "(BIV, BIV_final) = CSV_Formatter('Intermediate_Bonds', 'BIV')\n",
    "(HYG, HYG_final) = CSV_Formatter('Intermediate_Bonds', 'HYG')\n",
    "(IEF, IEF_final) = CSV_Formatter('Intermediate_Bonds', 'IEF')\n",
    "(IEI, IEI_final) = CSV_Formatter('Intermediate_Bonds', 'IEI')\n",
    "(IGIB, IGIB_final) = CSV_Formatter('Intermediate_Bonds', 'IGIB')\n",
    "(IPE, IPE_final) = CSV_Formatter('Intermediate_Bonds', 'IPE')\n",
    "(ITE, ITE_final) = CSV_Formatter('Intermediate_Bonds', 'ITE')\n",
    "(TIP, TIP_final) = CSV_Formatter('Intermediate_Bonds', 'TIP')\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "df_itb_values = [BIV[['BIV']], HYG[['HYG']], IEF[['IEF']], IEI[['IEI']], IGIB[['IGIB']], IPE[['IPE']], ITE[['ITE']], TIP[['TIP']]]\n",
    "df_itb_finals = [BIV_final, HYG_final, IEF_final, IEI_final, IGIB_final, IPE_final, ITE_final, TIP_final]\n",
    "itb_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_itb_values)\n",
    "itb_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_itb_finals)\n",
    "\n",
    "itb_weekly = weekly_master(itb_increments_df, '2019-02-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = itb_weekly\n",
    "\n",
    "df = df[['BIV', 'HYG', 'IEF', 'IEI', 'IGIB', 'IPE', 'ITE', 'TIP']]\n",
    "\n",
    "df['Average'] = (df['BIV'] + df['HYG'] + df['IEF'] + df['IEI'] + df['IGIB'] + df['IPE'] + df['ITE'] + df['TIP'])/8\n",
    "\n",
    "itb = pd.DataFrame()\n",
    "\n",
    "itb.index_name = df.index.name\n",
    "itb['Itb_increments'] = df.Average\n",
    "itb.index = df.index\n",
    "\n",
    "itb_2 = timed_eval('ITB', itb, 2)\n",
    "itb_3 = timed_eval('ITB', itb_2, 3)\n",
    "itb_6 = timed_eval('ITB', itb_3, 6)\n",
    "itb_9 = timed_eval('ITB', itb_6, 9)\n",
    "itb_12 = timed_eval('ITB', itb_9, 12)\n",
    "itb_df = timed_eval('ITB', itb_12, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files for Long Term Bonds #\n",
    "\n",
    "(PRULX, PRULX_final) = CSV_Formatter('Long_Term_Bonds', 'PRULX')\n",
    "(VUSTX, VUSTX_final) = CSV_Formatter('Long_Term_Bonds', 'VUSTX')\n",
    "(WHOSX, WHOSX_final) = CSV_Formatter('Long_Term_Bonds', 'WHOSX')\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "df_ltb_values = [PRULX[['PRULX']], VUSTX[['VUSTX']], WHOSX[['WHOSX']]]\n",
    "df_ltb_finals = [PRULX_final, VUSTX_final, WHOSX_final]\n",
    "ltb_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_ltb_values)\n",
    "ltb_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_ltb_finals)\n",
    "\n",
    "ltb_weekly = weekly_master(ltb_increments_df, '2019-02-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ltb_weekly\n",
    "\n",
    "df = df[['PRULX', 'VUSTX', 'WHOSX']]\n",
    "\n",
    "df['Average'] = (df['PRULX'] + df['VUSTX'] + df['WHOSX'])/3\n",
    "\n",
    "ltb = pd.DataFrame()\n",
    "\n",
    "ltb.index_name = df.index.name\n",
    "ltb['Ltb_increments'] = df.Average\n",
    "ltb.index = df.index\n",
    "\n",
    "ltb_2 = timed_eval('LTB', ltb, 2)\n",
    "ltb_3 = timed_eval('LTB', ltb_2, 3)\n",
    "ltb_6 = timed_eval('LTB', ltb_3, 6)\n",
    "ltb_9 = timed_eval('LTB', ltb_6, 9)\n",
    "ltb_12 = timed_eval('LTB', ltb_9, 12)\n",
    "ltb_df = timed_eval('LTB', ltb_12, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files for Gold \n",
    "\n",
    "(INIVX, INIVX_final) = CSV_Formatter('Gold', 'INIVX')\n",
    "(OPGSX, OPGSX_final) = CSV_Formatter('Gold', 'OPGSX')\n",
    "(SGGDX, SGGDX_final) = CSV_Formatter('Gold', 'SGGDX')\n",
    "(USERX, USERX_final) = CSV_Formatter('Gold', 'USERX')\n",
    "(VGPMX, VGPMX_final) = CSV_Formatter('Gold', 'VGPMX')\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "dfgold_values = [INIVX[['INIVX']], OPGSX[['OPGSX']], SGGDX[['SGGDX']], USERX[['USERX']], VGPMX[['VGPMX']]]\n",
    "dfgold_finals = [INIVX_final, OPGSX_final, SGGDX_final, USERX_final, VGPMX_final]\n",
    "gold_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), dfgold_values)\n",
    "gold_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), dfgold_finals)\n",
    "\n",
    "gold_weekly = weekly_master(gold_increments_df, '2019-02-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gold_weekly\n",
    "\n",
    "df = df[['INIVX', 'OPGSX', 'SGGDX', 'USERX', 'VGPMX']]\n",
    "\n",
    "df['Average'] = (df['INIVX'] + df['OPGSX'] + df['SGGDX'] + df['USERX'] + df['VGPMX'])/5\n",
    "\n",
    "gold = pd.DataFrame()\n",
    "\n",
    "gold.index_name = df.index.name\n",
    "gold['Gold_increments'] = df.Average\n",
    "gold.index = df.index\n",
    "\n",
    "gold_2 = timed_eval('Gold', gold, 2)\n",
    "gold_3 = timed_eval('Gold', gold_2, 3)\n",
    "gold_6 = timed_eval('Gold', gold_3, 6)\n",
    "gold_9 = timed_eval('Gold', gold_6, 9)\n",
    "gold_12 = timed_eval('Gold', gold_9, 12)\n",
    "gold_df = timed_eval('Gold', gold_12, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files from their respective download folders for Broad Basket Commodities #\n",
    "\n",
    "(DBC, DBC_final) = CSV_Formatter('Broad_Commodities', 'DBC')\n",
    "(DJP, DJP_final) = CSV_Formatter('Broad_Commodities', 'DJP')\n",
    "(GSG, GSG_final) = CSV_Formatter('Broad_Commodities', 'GSG')\n",
    "(GSP, GSP_final) = CSV_Formatter('Broad_Commodities', 'GSP')\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "df_commod_values = [DBC[['DBC']], DJP[['DJP']], GSG[['GSG']], GSP[['GSP']]]\n",
    "df_commod_finals = [DBC_final, DJP_final, GSG_final, GSP_final]\n",
    "commod_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_commod_values)\n",
    "commod_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_commod_finals)\n",
    "\n",
    "commod_weekly = weekly_master(commod_increments_df, '2019-02-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = commod_weekly\n",
    "\n",
    "df = df[['DBC', 'DJP', 'GSG', 'GSP']]\n",
    "\n",
    "df['Average'] = (df['DBC'] + df['DJP'] + df['GSG'] + df['GSP'])/4\n",
    "\n",
    "commod = pd.DataFrame()\n",
    "\n",
    "commod.index_name = df.index.name\n",
    "commod['Commodity_increments'] = df.Average\n",
    "commod.index = df.index\n",
    "\n",
    "commod_2 = timed_eval('Commod', commod, 2)\n",
    "commod_3 = timed_eval('Commod', commod_2, 3)\n",
    "commod_6 = timed_eval('Commod', commod_3, 6)\n",
    "commod_9 = timed_eval('Commod', commod_6, 9)\n",
    "commod_12 = timed_eval('Commod', commod_9, 12)\n",
    "commod_df = timed_eval('Commod', commod_12, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "dfs = [stocks_df, itb_df, ltb_df, gold_df, commod_df]\n",
    "\n",
    "for df in dfs:\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,on='Date'), dfs)\n",
    "\n",
    "full_features = df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing recommendation values using Tuned ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices, percentages = CSV_Formatter('Stock_Indices', 'VIGRX')\n",
    "\n",
    "weeks = weekly_master(percentages, '2019-02-01')\n",
    "weeks_2 = timed_eval('VIGRX', weeks, 2)\n",
    "weeks_3 = timed_eval('VIGRX', weeks_2, 3)\n",
    "weeks_6 = timed_eval('VIGRX', weeks_3, 6)\n",
    "weeks_9 = timed_eval('VIGRX', weeks_6, 9)\n",
    "weeks_12 = timed_eval('VIGRX', weeks_9, 12)\n",
    "weeks_df = timed_eval('VIGRX', weeks_12, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical = pd.merge(df_final, weeks_df, on='Date')\n",
    "\n",
    "#historical.to_csv('csv_files/historical_record.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
