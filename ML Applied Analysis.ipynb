{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Applied Analysis\n",
    "Steve Donahue, www.github.com/sdonahue0132\n",
    "\n",
    "This Notebook makes an initial analysis of how useful the trained ML models are at guiding an investor in managing their All Weather Portfolio. Data for this notebook comes from the ML Preprocessing notebook and the algorithms used were developed in the ML SMOTE and ML ADASYN notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries #\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import math\n",
    "import datetime\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from functools import reduce\n",
    "import warnings\n",
    "from random import randint\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV_Formatter prepares a dataframe of values and a dataframe of percentages for each date of the fund's history. \n",
    "# It cleans the csv by isolating one measurement per day, at the Close of business, and then creating\n",
    "# a shifted column for comparison and creating a column of the daily percent change in value. \n",
    "# Both daily value and percent change are retained in two separate dataframes. \n",
    "\n",
    "def CSV_Formatter (folder_name, filename):\n",
    "    df_init = pd.read_csv('raw_data/' + str(folder_name) +'/' + str(filename) +'.csv')\n",
    "    df = df_init.set_index('Date')[['Close']]\n",
    "    df.columns = [str(filename)]\n",
    "    df['Next Day Values'] = df[filename].shift(-1)\n",
    "    df['Percentages'] = df['Next Day Values']/df[str(filename)]\n",
    "    df_final = df[['Percentages']]\n",
    "    df_final.columns = [str(filename)]\n",
    "    return(df[[filename]], df_final)\n",
    "    \n",
    "def clean_label (labels):\n",
    "    cleaned = str(labels).strip(\"Index'\").replace(\"_y'\", '').replace('Index([', '').replace(']', '').replace(\n",
    "        \", dtype='object')\", '').replace('[', '').replace(\"'\", '').replace(\"_x'\", '').replace('\"', '')\n",
    "    return cleaned\n",
    "\n",
    "# The last function is a weekly master function which collects the percent changes week to week for a domain dataframe\n",
    "# Domain dataframe should consist only of a date time index and a value series\n",
    "\n",
    "def weekly_master(df, cutoff_date):\n",
    "    \n",
    "# A little setup is necessary to ensure fidelity across weekly data.  In the original yahoo finance data downloads,\n",
    "# Weekends and holidays are not counted in the Datetime Index.  The following code creates a working df that\n",
    "# Can be broken into calendar weeks at regular 7 day intervals, to better reflect paycheck contributions and \n",
    "# subsequent analyses can be done on a week to week basis.\n",
    "\n",
    "# Note that for days where data is unavailable, I've filled in the value 1, since the method of assessing portfolios\n",
    "# Is multiplication across daily percentage changes.  In this way, days when no percent changes are documented do not\n",
    "# affect the value of the investment.\n",
    "\n",
    "    # NOTE THAT THIS FUNCTION WILL ONLY ACCOMODATE DATAFRAMES WITH UP TO 8 COLUMNS AS WRITTEN! #\n",
    "\n",
    "    cutoff_date = pd.to_datetime(cutoff_date)\n",
    "    cutoff = cutoff_date - datetime.timedelta(days=cutoff_date.weekday())\n",
    "    \n",
    "    if cutoff_date.weekday() == 6:\n",
    "        td = timedelta(1)\n",
    "        cutoff_date = cutoff_date - td\n",
    "    \n",
    "    datelist = pd.to_datetime(df.index.values)\n",
    "    df['Datetime'] = datelist\n",
    "    df_timed = df.set_index('Datetime')\n",
    "    labels = df_timed.columns\n",
    "\n",
    "    df_segmented = pd.DataFrame(columns = labels)\n",
    "\n",
    "    daterange = int(str(df_timed.index.max() - df_timed.index.min()).replace(\" days 00:00:00\", ''))\n",
    "    all_dates = pd.date_range(df_timed.index.min(), periods=daterange).tolist()\n",
    "    \n",
    "    index_df = pd.DataFrame(all_dates)\n",
    "    index_df.columns = ['Datetime']\n",
    "    \n",
    "    working_df = index_df.merge(df_timed, how = 'outer', left_on = 'Datetime', right_on ='Datetime')\n",
    "    working_df = working_df.fillna(1).set_index('Datetime').sort_values('Datetime', ascending = False)\n",
    "    working_df = working_df.loc[working_df.index <= cutoff_date]\n",
    "    \n",
    "    days = len(working_df.index)\n",
    "    number_of_weeks = int(np.floor(days/7))\n",
    "\n",
    "    df_progress = pd.DataFrame(index=[0,1,2,3,4])\n",
    "    weekly_eval = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, number_of_weeks):\n",
    "        portfolio_segment = working_df.iloc[i*7:(i+1)*7-1]\n",
    "        \n",
    "        products =[]\n",
    "        prod_1 = portfolio_segment.iloc[:, 0].product()\n",
    "        products.append(prod_1)\n",
    "\n",
    "        if len(labels) > 1:\n",
    "            prod_2 = portfolio_segment.iloc[:, 1].product()\n",
    "            products.append(prod_2)\n",
    "        if len(labels) > 2:        \n",
    "            prod_3 = portfolio_segment.iloc[:, 2].product()\n",
    "            products.append(prod_3)\n",
    "        if len(labels) > 3:\n",
    "            prod_4 = portfolio_segment.iloc[:, 3].product()\n",
    "            products.append(prod_4)\n",
    "        if len(labels) > 4:\n",
    "            prod_5 = portfolio_segment.iloc[:, 4].product()\n",
    "            products.append(prod_5)\n",
    "        if len(labels) > 5:\n",
    "            prod_6 = portfolio_segment.iloc[:, 5].product()\n",
    "            products.append(prod_6)\n",
    "        if len(labels) > 6:\n",
    "            prod_7 = portfolio_segment.iloc[:, 6].product()\n",
    "            products.append(prod_7)\n",
    "        if len(labels) > 7:\n",
    "            prod_8 = portfolio_segment.iloc[:, 7].product()\n",
    "            products.append(prod_8)\n",
    "   \n",
    "    # Can we just make this into a loop?\n",
    "    # For j in range(0, len(labels)):\n",
    "    #    temp_prod = portfolio_segment.iloc[:, j].product()\n",
    "    #    products.append(temp_prod)\n",
    "\n",
    "\n",
    "        weekly_eval[str(portfolio_segment.index[cutoff_date.weekday()]).replace(\"00:00:00\", '')] = products\n",
    "    \n",
    "    weekly_eval = weekly_eval.T\n",
    "    weekly_eval.columns = labels\n",
    "    weekly_eval.index = pd.to_datetime(weekly_eval.index)\n",
    "    weekly_eval.index.name = 'Date'\n",
    "    weekly_eval.sort_index()\n",
    "\n",
    "    return(weekly_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fidelity US 500, 'FUSEX', was rolled into another index, 'FXAIX' in November 2018.  \n",
    "# This cell joins the two records at the transition point for.... fidelity.\n",
    "\n",
    "(FXAIX, FXAIX_final) = CSV_Formatter('Stock_Indices', 'FXAIX')\n",
    "(FUSEX, FUSEX_final) = CSV_Formatter('Stock_Indices', 'FUSEX')\n",
    "\n",
    "FUSEX = pd.merge( FUSEX_final, FXAIX_final, how = 'outer', on = 'Date')\n",
    "\n",
    "FUSEX = pd.DataFrame(FUSEX['FUSEX'].fillna(FUSEX['FXAIX']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files for STOCK INDICES #\n",
    "\n",
    "(PREIX, PREIX_final) = CSV_Formatter('Stock_Indices', 'PREIX')\n",
    "(SWPPX, SWPPX_final) = CSV_Formatter('Stock_Indices', 'SWPPX')\n",
    "#(FUSEX, FUSEX_final) = CSV_Formatter('Stock_Indices', 'FUSEX_plus')\n",
    "(VFINX, VFINX_final) = CSV_Formatter('Stock_Indices', 'VFINX')\n",
    "(VIGRX, VIGRX_final) = CSV_Formatter('Stock_Indices', 'VIGRX')\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "dfstock_values = [PREIX[['PREIX']], SWPPX[['SWPPX']], FUSEX[['FUSEX']], VFINX[['VFINX']], VIGRX[['VIGRX']]]\n",
    "dfstock_finals = [PREIX_final, SWPPX_final, FUSEX, VFINX_final, VIGRX_final]\n",
    "\n",
    "#stock_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), dfstock_values)\n",
    "stock_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), dfstock_finals)\n",
    "\n",
    "pd.merge(stock_increments_df, FUSEX_final, how = 'outer')\n",
    "stock_weekly = weekly_master(stock_increments_df, '2019-02-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files for Intermediate Term Bonds #\n",
    "\n",
    "(BIV, BIV_final) = CSV_Formatter('Intermediate_Bonds', 'BIV')\n",
    "(HYG, HYG_final) = CSV_Formatter('Intermediate_Bonds', 'HYG')\n",
    "(IEF, IEF_final) = CSV_Formatter('Intermediate_Bonds', 'IEF')\n",
    "(IEI, IEI_final) = CSV_Formatter('Intermediate_Bonds', 'IEI')\n",
    "(IGIB, IGIB_final) = CSV_Formatter('Intermediate_Bonds', 'IGIB')\n",
    "(IPE, IPE_final) = CSV_Formatter('Intermediate_Bonds', 'IPE')\n",
    "(ITE, ITE_final) = CSV_Formatter('Intermediate_Bonds', 'ITE')\n",
    "(TIP, TIP_final) = CSV_Formatter('Intermediate_Bonds', 'TIP')\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "df_itb_values = [BIV[['BIV']], HYG[['HYG']], IEF[['IEF']], IEI[['IEI']], IGIB[['IGIB']], IPE[['IPE']], ITE[['ITE']], TIP[['TIP']]]\n",
    "df_itb_finals = [BIV_final, HYG_final, IEF_final, IEI_final, IGIB_final, IPE_final, ITE_final, TIP_final]\n",
    "itb_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_itb_values)\n",
    "itb_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_itb_finals)\n",
    "\n",
    "itb_weekly = weekly_master(itb_increments_df, '2019-02-01')\n",
    "\n",
    "#itb_weekly.to_csv('itb_increments_training.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files for Long Term Bonds #\n",
    "\n",
    "(PRULX, PRULX_final) = CSV_Formatter('Long_Term_Bonds', 'PRULX')\n",
    "(VUSTX, VUSTX_final) = CSV_Formatter('Long_Term_Bonds', 'VUSTX')\n",
    "(WHOSX, WHOSX_final) = CSV_Formatter('Long_Term_Bonds', 'WHOSX')\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "df_ltb_values = [PRULX[['PRULX']], VUSTX[['VUSTX']], WHOSX[['WHOSX']]]\n",
    "df_ltb_finals = [PRULX_final, VUSTX_final, WHOSX_final]\n",
    "ltb_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_ltb_values)\n",
    "ltb_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_ltb_finals)\n",
    "\n",
    "ltb_weekly = weekly_master(ltb_increments_df, '2019-02-01')\n",
    "\n",
    "#ltb_weekly.to_csv('ltb_increments_training.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files for Gold \n",
    "\n",
    "(INIVX, INIVX_final) = CSV_Formatter('Gold', 'INIVX')\n",
    "(OPGSX, OPGSX_final) = CSV_Formatter('Gold', 'OPGSX')\n",
    "(SGGDX, SGGDX_final) = CSV_Formatter('Gold', 'SGGDX')\n",
    "(USERX, USERX_final) = CSV_Formatter('Gold', 'USERX')\n",
    "(VGPMX, VGPMX_final) = CSV_Formatter('Gold', 'VGPMX')\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "dfgold_values = [INIVX[['INIVX']], OPGSX[['OPGSX']], SGGDX[['SGGDX']], USERX[['USERX']], VGPMX[['VGPMX']]]\n",
    "dfgold_finals = [INIVX_final, OPGSX_final, SGGDX_final, USERX_final, VGPMX_final]\n",
    "gold_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), dfgold_values)\n",
    "gold_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), dfgold_finals)\n",
    "\n",
    "gold_weekly = weekly_master(gold_increments_df, '2019-02-01')\n",
    "\n",
    "#gold_weekly.to_csv('gold_increments_training.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell reads all necessary source files from their respective download folders for Broad Basket Commodities #\n",
    "\n",
    "(DBC, DBC_final) = CSV_Formatter('Broad_Commodities', 'DBC')\n",
    "(DJP, DJP_final) = CSV_Formatter('Broad_Commodities', 'DJP')\n",
    "(GSG, GSG_final) = CSV_Formatter('Broad_Commodities', 'GSG')\n",
    "(GSP, GSP_final) = CSV_Formatter('Broad_Commodities', 'GSP')\n",
    "\n",
    "# Creates dataframes of their daily values and daily percent changes , aka increments.\n",
    "df_commod_values = [DBC[['DBC']], DJP[['DJP']], GSG[['GSG']], GSP[['GSP']]]\n",
    "df_commod_finals = [DBC_final, DJP_final, GSG_final, GSP_final]\n",
    "commod_daily_values_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_commod_values)\n",
    "commod_increments_df = reduce(lambda left, right: pd.merge(left, right, on = 'Date'), df_commod_finals)\n",
    "\n",
    "commod_weekly = weekly_master(commod_increments_df, '2019-02-01')\n",
    "\n",
    "#commod_weekly.to_csv('commod_weekly_training.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time weighted return captures an accurate total return, accounting for time of accrual.\n",
    "\n",
    "def time_weighted_return (df):\n",
    "    sorted_df = df.sort_index(ascending=True).dropna()\n",
    "    sorted_df['after_cash_flows'] = sorted_df.ending_value + sorted_df.cash_flows\n",
    "    sorted_df['prev_after_cash_flows'] = sorted_df.after_cash_flows.shift(1)\n",
    "    sorted_df['HPR'] = (sorted_df.ending_value / sorted_df.prev_after_cash_flows) - 1\n",
    "    sorted_df['HPR_plus_one'] = sorted_df.HPR + 1\n",
    "    sorted_df = sorted_df.dropna()\n",
    "\n",
    "    rate = sorted_df.HPR_plus_one.product()-1\n",
    "    \n",
    "    return (rate)\n",
    " \n",
    "# This function is used to compute the Compounded Annual Growth Rate and return the identities and length of investment\n",
    "\n",
    "def p_summary(performance, fund_identities, term_years):\n",
    "    twr = time_weighted_return(performance)\n",
    "    CAGR = (float((1+twr))**(float(1/term_years)) -1)\n",
    "    return(CAGR, fund_identities, term_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_tester(y_predict, funds_moved_ratio):\n",
    "\n",
    "    stock_name, itb_name, ltb_name, gold_name, commod_name = 'VIGRX', 'IEF', 'WHOSX', 'SGGDX', 'DBC'\n",
    "\n",
    "    contribution = 500\n",
    "\n",
    "    # Extract the columns of percent changes \n",
    "    rand_stock_inc = stock_weekly[[stock_name]]\n",
    "    rand_inter_bond_inc = itb_weekly[[itb_name]]\n",
    "    rand_long_bond_inc = ltb_weekly[[ltb_name]]\n",
    "    rand_gold_inc = gold_weekly[[gold_name]]\n",
    "    rand_commod_inc = commod_weekly[[commod_name]]\n",
    "\n",
    "    random_inc = rand_stock_inc.merge(rand_inter_bond_inc, on = 'Date').merge(\n",
    "        rand_long_bond_inc, on = 'Date').merge(\n",
    "        rand_gold_inc, on = 'Date').merge(\n",
    "        rand_commod_inc, on = 'Date')\n",
    "\n",
    "    test = random_inc.dropna()    \n",
    "    test = test.sort_index(ascending = True)\n",
    "    test_col = test.iloc[1:, 0:5]\n",
    "    num_weeks = test_col.shape[0]\n",
    "\n",
    "    # This block sets markers in the test dataframe for when to add contributions, and ML flags for redistro\n",
    "    a = np.empty((num_weeks,))\n",
    "    a[::2] = 0\n",
    "    a[1::2] = 1\n",
    "    a = a*contribution\n",
    "    ML = y_predict[::-1]\n",
    "    test_col['contributions'] = a\n",
    "    test_col['ML_flag'] = ML\n",
    "\n",
    "    # This block determines the portions of the contributions to allocate to each fund.\n",
    "    invesment_stock = contribution*0.3\n",
    "    invesment_itb = contribution*0.4\n",
    "    invesment_ltb = contribution*0.15\n",
    "    invesment_gold = contribution*0.075\n",
    "    invesment_commod = contribution*0.075\n",
    "\n",
    "    values = test_col.values\n",
    "\n",
    "    # Progress lists are used to accumulate the values of the incremented investments.  Fund Identities are formatted\n",
    "    progress_stock = []\n",
    "    progress_itb = []\n",
    "    progress_ltb = []\n",
    "    progress_gold = []\n",
    "    progress_commod = []\n",
    "    labels = str(test.columns)\n",
    "    fund_identities = str(labels).strip('Index').replace('_x', '').replace('([', '').replace(\"], dtype='object')\", '')\n",
    "\n",
    "    # This loop applies the increments to the investment amounts, and appends the results to the progress lists\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for i,j,k, l, m, contribution, flag in values:\n",
    "\n",
    "        if flag == 0:\n",
    "            funds_moved = invesment_stock * funds_moved_ratio\n",
    "        \n",
    "            invesment_stock = invesment_stock - funds_moved\n",
    "            invesment_itb = invesment_itb + funds_moved/4\n",
    "            invesment_ltb = invesment_ltb + funds_moved/4\n",
    "            invesment_gold = invesment_gold + funds_moved/4\n",
    "            invesment_commod = invesment_commod + funds_moved/4\n",
    "    \n",
    "        if flag == 1:\n",
    "            subtotal = invesment_stock + invesment_itb + invesment_ltb + invesment_gold + invesment_commod\n",
    "            stock_prop = invesment_stock/subtotal\n",
    "            if stock_prop < 0.30:\n",
    "                invesment_stock = subtotal*0.3\n",
    "                invesment_itb = subtotal*0.4\n",
    "                invesment_ltb = subtotal*0.15\n",
    "                invesment_gold = subtotal*0.075\n",
    "                invesment_commod = subtotal*0.075\n",
    "    \n",
    "        invesment_stock = invesment_stock*i + contribution*0.3\n",
    "        invesment_itb = invesment_itb*j + contribution*0.4\n",
    "        invesment_ltb = invesment_ltb*k + contribution*0.15\n",
    "        invesment_gold = invesment_gold*l + contribution*0.075\n",
    "        invesment_commod = invesment_commod*m + contribution*0.075\n",
    "    \n",
    "        if counter % 52 == 0:\n",
    "            invesment_stock = subtotal*0.3\n",
    "            invesment_itb = subtotal*0.4\n",
    "            invesment_ltb = subtotal*0.15\n",
    "            invesment_gold = subtotal*0.075\n",
    "            invesment_commod = subtotal*0.075\n",
    "        \n",
    "        progress_stock.append(invesment_stock)\n",
    "        progress_itb.append(invesment_itb)\n",
    "        progress_ltb.append(invesment_ltb)\n",
    "        progress_gold.append(invesment_gold)\n",
    "        progress_commod.append(invesment_commod)\n",
    "    \n",
    "        counter = counter + 1\n",
    "    \n",
    "    # Lists are converted to arrays, and placed into a performace dataframe, which is evaluated for annualized return %\n",
    "    result_stock = np.array(progress_stock)\n",
    "    result_itb = np.array(progress_itb)\n",
    "    result_ltb = np.array(progress_ltb)\n",
    "    result_gold = np.array(progress_gold)\n",
    "    result_commod = np.array(progress_commod)\n",
    "\n",
    "    test_col['Stock_Eval'] = result_stock\n",
    "    test_col['ITB_Eval']= result_itb\n",
    "    test_col['LTB_Eval'] = result_ltb\n",
    "    test_col['Gold_Eval'] = result_gold\n",
    "    test_col['Commod_Eval']= result_commod\n",
    "\n",
    "    performance = test_col.iloc[:, 7:12]\n",
    "    performance['ending_value'] = performance.sum(axis=1)\n",
    "    performance['cash_flows'] = a\n",
    "    performance['ML_flags'] = ML\n",
    "    performance = performance.sort_index(ascending = False)\n",
    "    term_years = float(len(performance.index)/52.17857)\n",
    "\n",
    "    CAGR, ident, years = p_summary(performance, fund_identities, term_years)\n",
    "\n",
    "    return(round(CAGR*100,2), performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pd.read_csv('csv_files/historical_record.csv')\n",
    "processed.index = pd.to_datetime(processed.index)\n",
    "processed = processed.iloc[35:, :]\n",
    "\n",
    "processed.head()\n",
    "\n",
    "processed.reset_index()\n",
    "processed.index = processed['Date']\n",
    "processed = processed.drop('Date', axis = 1)\n",
    "\n",
    "processed = processed[['VIGRX', 'Stocks 2_Week_Avg', 'Stocks 3_Week_Avg',\n",
    "       'Stocks 6_Week_Avg', 'Stocks 9_Week_Avg', 'Stocks 12_Week_Avg',\n",
    "       'Stocks 15_Week_Avg',  'ITB 2_Week_Avg',\n",
    "       'ITB 3_Week_Avg', 'ITB 6_Week_Avg', 'ITB 9_Week_Avg', 'ITB 12_Week_Avg',\n",
    "       'ITB 15_Week_Avg', 'LTB 2_Week_Avg', 'LTB 3_Week_Avg',\n",
    "       'LTB 6_Week_Avg', 'LTB 9_Week_Avg', 'LTB 12_Week_Avg',\n",
    "       'LTB 15_Week_Avg',  'Gold 2_Week_Avg',\n",
    "       'Gold 3_Week_Avg', 'Gold 6_Week_Avg', 'Gold 9_Week_Avg',\n",
    "       'Gold 12_Week_Avg', 'Gold 15_Week_Avg', \n",
    "       'Commod 2_Week_Avg', 'Commod 3_Week_Avg', 'Commod 6_Week_Avg',\n",
    "       'Commod 9_Week_Avg', 'Commod 12_Week_Avg', 'Commod 15_Week_Avg',\n",
    "        'VIGRX 2_Week_Avg', 'VIGRX 3_Week_Avg', 'VIGRX 6_Week_Avg',\n",
    "       'VIGRX 9_Week_Avg', 'VIGRX 12_Week_Avg', 'VIGRX 15_Week_Avg']]\n",
    "\n",
    "full_features = processed\n",
    "\n",
    "for_testing = pd.read_csv('csv_files/historical_record.csv')\n",
    "for_testing.index = pd.to_datetime(for_testing.index)\n",
    "\n",
    "for_testing.reset_index()\n",
    "for_testing.index = for_testing['Date']\n",
    "for_testing = for_testing.drop('Date', axis = 1)\n",
    "\n",
    "for_testing = for_testing[['VIGRX', 'Stocks 2_Week_Avg', 'Stocks 3_Week_Avg',\n",
    "       'Stocks 6_Week_Avg', 'Stocks 9_Week_Avg', 'Stocks 12_Week_Avg',\n",
    "       'Stocks 15_Week_Avg',  'ITB 2_Week_Avg',\n",
    "       'ITB 3_Week_Avg', 'ITB 6_Week_Avg', 'ITB 9_Week_Avg', 'ITB 12_Week_Avg',\n",
    "       'ITB 15_Week_Avg', 'LTB 2_Week_Avg', 'LTB 3_Week_Avg',\n",
    "       'LTB 6_Week_Avg', 'LTB 9_Week_Avg', 'LTB 12_Week_Avg',\n",
    "       'LTB 15_Week_Avg',  'Gold 2_Week_Avg',\n",
    "       'Gold 3_Week_Avg', 'Gold 6_Week_Avg', 'Gold 9_Week_Avg',\n",
    "       'Gold 12_Week_Avg', 'Gold 15_Week_Avg', \n",
    "       'Commod 2_Week_Avg', 'Commod 3_Week_Avg', 'Commod 6_Week_Avg',\n",
    "       'Commod 9_Week_Avg', 'Commod 12_Week_Avg', 'Commod 15_Week_Avg',\n",
    "        'VIGRX 2_Week_Avg', 'VIGRX 3_Week_Avg', 'VIGRX 6_Week_Avg',\n",
    "       'VIGRX 9_Week_Avg', 'VIGRX 12_Week_Avg', 'VIGRX 15_Week_Avg']]\n",
    "\n",
    "test_features = for_testing\n",
    "\n",
    "test_obs = test_features.iloc[:, 1:37]\n",
    "t_obs = test_obs.transpose()\n",
    "\n",
    "X_testing = []\n",
    "\n",
    "for i in range(t_obs.shape[1]):\n",
    "    x_testing = t_obs.iloc[:,i].values\n",
    "    X_testing.append(x_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nina/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/nina/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/nina/anaconda3/lib/python3.6/importlib/_bootstrap_external.py:426: ImportWarning: Not importing directory /Users/nina/anaconda3/lib/python3.6/site-packages/mpl_toolkits: missing __init__\n",
      "  _warnings.warn(msg.format(portions[0]), ImportWarning)\n",
      "/Users/nina/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Identify the training data and targets\n",
    "\n",
    "training_df = full_features\n",
    "observations = training_df.iloc[:, 1:37]\n",
    "obs = observations.transpose()\n",
    "\n",
    "X = []\n",
    "\n",
    "for i in range(obs.shape[1]):\n",
    "    x = obs.iloc[:,i].values\n",
    "    X.append(x)\n",
    "\n",
    "y = training_df.iloc[:, 0].values\n",
    "\n",
    "#Resolve the target column as binary\n",
    "for i in range(len(y)):\n",
    "    if y[i] <= 0.98:\n",
    "        y[i] = 0\n",
    "    if y[i] > 0.98:\n",
    "        y[i] = 1\n",
    "\n",
    "X_ADASYN, y_ADASYN = ADASYN().fit_resample(X, y)\n",
    "X_SMOTE, y_SMOTE = SMOTE().fit_resample(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nina/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - ADASYN analysis\n",
      "For funds_moved_ratio,  0.0 CAGR =  9.88\n",
      "\n",
      "Random Forest - ADASYN analysis\n",
      "For funds_moved_ratio,  0.05 CAGR =  9.29\n",
      "\n",
      "Random Forest - ADASYN analysis\n",
      "For funds_moved_ratio,  0.1 CAGR =  9.14\n",
      "\n",
      "Random Forest - ADASYN analysis\n",
      "For funds_moved_ratio,  0.15 CAGR =  9.04\n",
      "\n",
      "Random Forest - ADASYN analysis\n",
      "For funds_moved_ratio,  0.2 CAGR =  8.91\n",
      "\n",
      "Random Forest - ADASYN analysis\n",
      "For funds_moved_ratio,  0.25 CAGR =  8.78\n",
      "\n",
      "Random Forest - ADASYN analysis\n",
      "For funds_moved_ratio,  0.3 CAGR =  8.64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfca = RandomForestClassifier(n_estimators = 25, max_depth = 14, min_samples_leaf = 0.25, \n",
    "                             min_samples_split = 0.1, max_features=4)\n",
    "\n",
    "rfca.fit(X_ADASYN, y_ADASYN)\n",
    "y_predict = rfca.predict(X_testing)\n",
    "\n",
    "for i in [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]:\n",
    "    CAGR, performance = ML_tester(y_predict, i)\n",
    "    print('Random Forest - ADASYN analysis')\n",
    "    print('For funds_moved_ratio, ', i, 'CAGR = ', CAGR)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nina/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Reg Classifier - ADASYN analysis\n",
      "For funds_moved_ratio,  0.0 CAGR =  10.13\n",
      "\n",
      "Log Reg Classifier - ADASYN analysis\n",
      "For funds_moved_ratio,  0.05 CAGR =  9.68\n",
      "\n",
      "Log Reg Classifier - ADASYN analysis\n",
      "For funds_moved_ratio,  0.1 CAGR =  9.5\n",
      "\n",
      "Log Reg Classifier - ADASYN analysis\n",
      "For funds_moved_ratio,  0.15 CAGR =  9.37\n",
      "\n",
      "Log Reg Classifier - ADASYN analysis\n",
      "For funds_moved_ratio,  0.2 CAGR =  9.24\n",
      "\n",
      "Log Reg Classifier - ADASYN analysis\n",
      "For funds_moved_ratio,  0.25 CAGR =  9.13\n",
      "\n",
      "Log Reg Classifier - ADASYN analysis\n",
      "For funds_moved_ratio,  0.3 CAGR =  9.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logrega = LogisticRegression()\n",
    "\n",
    "logrega.fit(X_ADASYN, y_ADASYN)\n",
    "\n",
    "y_predict = logrega.predict(X_testing)\n",
    "    \n",
    "for i in [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]:\n",
    "    CAGR, performance = ML_tester(y_predict, i)\n",
    "    print('Log Reg Classifier - ADASYN analysis')\n",
    "    print('For funds_moved_ratio, ', i, 'CAGR = ', CAGR)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nina/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Reg Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.0 CAGR =  10.18\n",
      "\n",
      "Log Reg Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.05 CAGR =  9.59\n",
      "\n",
      "Log Reg Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.1 CAGR =  9.37\n",
      "\n",
      "Log Reg Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.15 CAGR =  9.26\n",
      "\n",
      "Log Reg Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.2 CAGR =  9.17\n",
      "\n",
      "Log Reg Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.25 CAGR =  9.09\n",
      "\n",
      "Log Reg Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.3 CAGR =  9.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logregs = LogisticRegression()\n",
    "\n",
    "logregs.fit(X_SMOTE, y_SMOTE)\n",
    "\n",
    "y_predict = logregs.predict(X_testing)\n",
    "\n",
    "for i in [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]:\n",
    "    CAGR, performance = ML_tester(y_predict, i)\n",
    "    print('Log Reg Classifier - SMOTE analysis')\n",
    "    print('For funds_moved_ratio, ', i, 'CAGR = ', CAGR)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nina/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.0 CAGR=  10.43\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.05 CAGR=  10.21\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.1 CAGR=  10.13\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.15 CAGR=  10.06\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.2 CAGR=  9.99\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.25 CAGR=  9.94\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.3 CAGR=  9.91\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.35 CAGR=  9.88\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.4 CAGR=  9.86\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.45 CAGR=  9.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfcs_u = RandomForestClassifier()\n",
    "\n",
    "rfcs_u.fit(X_SMOTE, y_SMOTE)\n",
    "\n",
    "y_predict = rfcs_u.predict(X_testing)\n",
    "\n",
    "for i in [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]:\n",
    "    CAGR, performance = ML_tester(y_predict, i)\n",
    "    print('Random Forest Untuned Classifier - SMOTE analysis')\n",
    "    print('For funds_moved_ratio, ', i, 'CAGR= ', CAGR)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.0 CAGR=  10.0\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.05 CAGR=  9.42\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.1 CAGR=  9.27\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.15 CAGR=  9.15\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.2 CAGR=  9.03\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.25 CAGR=  8.91\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.3 CAGR=  8.79\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.35 CAGR=  8.67\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.4 CAGR=  8.57\n",
      "\n",
      "Random Forest Untuned Classifier - SMOTE analysis\n",
      "For funds_moved_ratio,  0.45 CAGR=  8.47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfcs_t = RandomForestClassifier(n_estimators = 9, max_depth = 11, min_samples_leaf = 0.2, \n",
    "                                 min_samples_split = 0.3, max_features=18)\n",
    "\n",
    "rfcs_t.fit(X_SMOTE, y_SMOTE)\n",
    "\n",
    "y_predict = rfcs_t.predict(X_testing)\n",
    "\n",
    "for i in [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]:\n",
    "    CAGR, performance = ML_tester(y_predict, i)\n",
    "    print('Random Forest Untuned Classifier - SMOTE analysis')\n",
    "    print('For funds_moved_ratio, ', i, 'CAGR= ', CAGR)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell develops an equally weighted voting system for all classifiers\n",
    "\n",
    "sum_of_predictors = rfca.predict(X_testing) + logrega.predict(X_testing) + logregs.predict(X_testing) + rfcs_u.predict(X_testing) + rfcs_t.predict(X_testing)\n",
    "\n",
    "list_version = list(sum_of_predictors)\n",
    "\n",
    "unanimous_1 = np.floor((sum_of_predictors)/5), '5 votes for 1'\n",
    "vote_four_1 = np.floor((sum_of_predictors)/4), '4 votes for 1'\n",
    "vote_three_1 = np.floor((sum_of_predictors)/3), '3 votes for 1'\n",
    "vote_two_1 = np.floor(np.array([2 if x in [3, 4, 5] else x for x in list_version])/2), '2 votes for 1'\n",
    "vote_one_1 = np.floor(np.array([1 if x in [2, 3, 4, 5] else x for x in list_version])), '1 vote for 1'\n",
    "\n",
    "all_methods = [unanimous_1, vote_four_1, vote_three_1, vote_two_1, vote_one_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.0 , CAGR =  9.77\n",
      "\n",
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.05 , CAGR =  9.02\n",
      "\n",
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.1 , CAGR =  8.76\n",
      "\n",
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.15 , CAGR =  8.61\n",
      "\n",
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.2 , CAGR =  8.44\n",
      "\n",
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.25 , CAGR =  8.3\n",
      "\n",
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.3 , CAGR =  8.16\n",
      "\n",
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.35 , CAGR =  8.05\n",
      "\n",
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.4 , CAGR =  7.95\n",
      "\n",
      "Using  5 votes for 1  method...\n",
      "Funds_moved_ratio =  0.45 , CAGR =  7.87\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.0 , CAGR =  9.9\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.05 , CAGR =  9.25\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.1 , CAGR =  9.08\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.15 , CAGR =  8.95\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.2 , CAGR =  8.82\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.25 , CAGR =  8.69\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.3 , CAGR =  8.56\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.35 , CAGR =  8.44\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.4 , CAGR =  8.32\n",
      "\n",
      "Using  4 votes for 1  method...\n",
      "Funds_moved_ratio =  0.45 , CAGR =  8.22\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.0 , CAGR =  10.17\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.05 , CAGR =  9.78\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.1 , CAGR =  9.63\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.15 , CAGR =  9.51\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.2 , CAGR =  9.4\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.25 , CAGR =  9.29\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.3 , CAGR =  9.19\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.35 , CAGR =  9.1\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.4 , CAGR =  9.02\n",
      "\n",
      "Using  3 votes for 1  method...\n",
      "Funds_moved_ratio =  0.45 , CAGR =  8.96\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.0 , CAGR =  10.17\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.05 , CAGR =  9.62\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.1 , CAGR =  9.41\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.15 , CAGR =  9.29\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.2 , CAGR =  9.2\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.25 , CAGR =  9.14\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.3 , CAGR =  9.08\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.35 , CAGR =  9.03\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.4 , CAGR =  8.99\n",
      "\n",
      "Using  2 votes for 1  method...\n",
      "Funds_moved_ratio =  0.45 , CAGR =  8.96\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.0 , CAGR =  10.39\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.05 , CAGR =  10.14\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.1 , CAGR =  10.03\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.15 , CAGR =  9.94\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.2 , CAGR =  9.86\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.25 , CAGR =  9.79\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.3 , CAGR =  9.72\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.35 , CAGR =  9.67\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.4 , CAGR =  9.62\n",
      "\n",
      "Using  1 vote for 1  method...\n",
      "Funds_moved_ratio =  0.45 , CAGR =  9.57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for votes, name in all_methods:\n",
    "    for i in [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45]:\n",
    "        CAGR, performance = ML_tester(votes, i)\n",
    "        print('Using ', name, ' method...')\n",
    "        print('Funds_moved_ratio = ', i, ', CAGR = ', CAGR)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
